# Synaptic Assistant — Model Roster, Voice & HTTP Configuration
#
# This file is the single source of truth for available LLM models, voice settings,
# and HTTP client parameters. Loaded at boot by Assistant.Config.Loader and
# hot-reloaded on file change.
#
# Env var interpolation: Use ${ENV_VAR} for sensitive values (resolved at load time).
# Unresolved env vars raise at boot; missing optional vars resolve to null.

# ---------------------------------------------------------------------------
# Defaults — which model tier to use for each system role
# ---------------------------------------------------------------------------
defaults:
  orchestrator: primary
  sub_agent: balanced
  compaction: fast
  sentinel: fast
  image_generation: balanced
  audio_transcription: fast     # STT — model that can process audio input
  audio_output: primary         # TTS via model (e.g. gpt-audio) — alternative/complement to ElevenLabs

# ---------------------------------------------------------------------------
# Model Roster — curated list of OpenRouter models
# ---------------------------------------------------------------------------
#
# Fields:
#   id              OpenRouter model identifier (provider/model-name)
#   tier            primary | balanced | fast | cheap
#   description     Human-readable summary
#   use_cases       List of roles this model is suitable for:
#                     orchestrator, sub_agent, compaction, sentinel,
#                     image_generation (can generate image output),
#                     audio_transcription (can process audio/speech input — STT),
#                     audio_output (can generate audio/speech output — TTS via model)
#   supports_tools  Whether the model supports tool/function calling
#   max_context_tokens  Maximum context window size
#   cost_tier       low | medium | high (relative cost classification)

models:
  - id: "anthropic/claude-sonnet-4-6"
    tier: primary
    description: "Claude Sonnet 4.6 — top-tier reasoning, tool calling, and instruction following"
    use_cases:
      - orchestrator
      - sub_agent
    supports_tools: true
    max_context_tokens: 200000
    cost_tier: high

  - id: "openai/gpt-5.2"
    tier: primary
    description: "GPT-5.2 — strong reasoning and tool calling, latest generation"
    use_cases:
      - orchestrator
      - sub_agent
    supports_tools: true
    max_context_tokens: 400000
    cost_tier: high

  # Image generation models
  - id: "openai/gpt-5-image"
    tier: primary
    description: "GPT-5 Image — highest quality OpenAI image generation"
    use_cases:
      - image_generation
    supports_tools: false
    max_context_tokens: 400000
    cost_tier: high

  - id: "openai/gpt-5-image-mini"
    tier: balanced
    description: "GPT-5 Image Mini — faster, lower-cost image generation"
    use_cases:
      - image_generation
    supports_tools: false
    max_context_tokens: 400000
    cost_tier: medium

  - id: "google/gemini-2.5-flash-image"
    tier: fast
    description: "Gemini 2.5 Flash Image — fast multimodal image generation"
    use_cases:
      - image_generation
    supports_tools: false
    max_context_tokens: 1048576
    cost_tier: low

  - id: "anthropic/claude-haiku-4-5-20251001"
    tier: fast
    description: "Claude Haiku 4.5 — fast and cheap, good for lightweight tasks"
    use_cases:
      - sub_agent
      - compaction
      - sentinel
    supports_tools: true
    max_context_tokens: 200000
    cost_tier: low

  - id: "openai/gpt-5-mini"
    tier: fast
    description: "GPT-5 Mini — very fast, low cost, multimodal including audio input"
    use_cases:
      - sub_agent
      - compaction
      - sentinel
    supports_tools: true
    max_context_tokens: 400000
    cost_tier: low

  # Audio-capable models — can transcribe speech input and/or generate speech output.
  # These complement ElevenLabs TTS; audio_output models can be used as an alternative
  # when native audio responses are preferred (e.g. lower latency, conversational voice).

  - id: "google/gemini-3-flash-preview"
    tier: balanced
    description: "Gemini 3 Flash Preview — fast, large context, audio input, $0.50/$3 per M tokens"
    use_cases:
      - orchestrator
      - sub_agent
      - compaction
      - audio_transcription
    supports_tools: true
    max_context_tokens: 1048576
    cost_tier: low

  - id: "google/gemini-3-pro-preview"
    tier: primary
    description: "Gemini 3 Pro Preview — top-tier reasoning, audio input/output, $2/$12 per M tokens"
    use_cases:
      - orchestrator
      - sub_agent
      - audio_transcription
      - audio_output
    supports_tools: true
    max_context_tokens: 1048576
    cost_tier: high

  - id: "openai/gpt-audio"
    tier: primary
    description: "GPT Audio — native audio input and output, 128k context, $2.50/$10 per M tokens, $32 audio tokens; alternative to ElevenLabs TTS"
    use_cases:
      - sub_agent
      - audio_transcription
      - audio_output
    supports_tools: true
    max_context_tokens: 128000
    cost_tier: high

  - id: "openai/gpt-audio-mini"
    tier: balanced
    description: "GPT Audio Mini — native audio input/output, 128k context, $0.60/$2.40 per M tokens, $0.60 audio tokens; low-cost audio alternative"
    use_cases:
      - sub_agent
      - audio_transcription
      - audio_output
    supports_tools: true
    max_context_tokens: 128000
    cost_tier: medium

# ---------------------------------------------------------------------------
# HTTP Client Configuration — OpenRouter request settings
# ---------------------------------------------------------------------------
#
# All operational HTTP parameters live here so you can tune without code changes.
# hot-reloadable via Config.Watcher.

http:
  max_retries: 3                # Maximum retry attempts on transient errors (429, 500, 502, 503, 504)
  base_backoff_ms: 1000         # Initial backoff before first retry (exponential with jitter)
  max_backoff_ms: 30000         # Cap on backoff duration
  request_timeout_ms: 120000    # Per-request timeout (2 min — long chains can take time)
  streaming_timeout_ms: 300000  # Timeout for streaming responses (5 min)

# ---------------------------------------------------------------------------
# Limits — orchestrator and sub-agent operational boundaries
# ---------------------------------------------------------------------------
#
# Token budgets, turn limits, and cache settings for context management.
# context_utilization_target * max_context_tokens - response_reserve_tokens
# = available token budget for conversation history.

limits:
  context_utilization_target: 0.85    # Use up to 85% of model's context window
  compaction_trigger_threshold: 0.75  # Trigger compaction at 75% utilization
  response_reserve_tokens: 4096      # Reserve for model's response
  orchestrator_turn_limit: 100       # Max turns per orchestrator conversation
  sub_agent_turn_limit: 30           # Max turns per sub-agent dispatch
  cache_ttl_seconds: 3600            # Default cache TTL (1 hour)
  orchestrator_cache_breakpoints: 4  # Max cache breakpoints for orchestrator
  sub_agent_cache_breakpoints: 1     # Max cache breakpoints for sub-agents

# ---------------------------------------------------------------------------
# Voice Configuration — ElevenLabs TTS settings
# ---------------------------------------------------------------------------
#
# voice_id uses env var interpolation for flexibility across environments.
# tts_model selects the ElevenLabs model (see API docs for options).
# optimize_streaming_latency: 0 (max quality) to 4 (min latency).
# voice_settings are optional overrides; ElevenLabs uses voice defaults if omitted.

voice:
  voice_id: "${ELEVENLABS_VOICE_ID}"
  tts_model: "eleven_flash_v2_5"
  optimize_streaming_latency: 3
  output_format: "mp3_44100_128"
  voice_settings:
    stability: 0.5
    similarity_boost: 0.75
    style: 0.0
    speed: 1.0
